{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvkvasanth/Spark/blob/main/Spark_Query_Plan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZIKnCIbLbGw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "sparkSession = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()\n",
        "sc = sparkSession.sparkContext\n",
        "sc.setLogLevel(\"INFO\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZjitdAp52Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sparkSession.read.parquet(\"Spark/data/data_skew/transactions.parquet\")"
      ],
      "metadata": {
        "id": "goDKIojt8-0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "e81b3917-fc50-4413-b636-b7a62fdfbdfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/Spark/data/data_skew/transactions.parquet.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-316145293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spark/data/data_skew/transactions.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/Spark/data/data_skew/transactions.parquet."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHzfSjEr9kM2",
        "outputId": "1743b962-4ec0-415a-a058-42ee0a149a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |city       |\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|10   |7  |Entertainment|10.42 |boston     |\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|3    |27 |Motor/Travel |44.34 |portland   |\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|4    |11 |Entertainment|3.18  |chicago    |\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|2    |22 |Groceries    |268.97|los_angeles|\n",
            "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|10   |16 |Entertainment|2.66  |chicago    |\n",
            "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df = sparkSession.read.parquet(\"Spark/data/data_skew/customers.parquet\")\n",
        "customer_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOQ4U_Hd-A5f",
        "outputId": "71d2abae-b877-4f54-bbf8-de2ba3854978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---+------+----------+-----+-----------+\n",
            "|   cust_id|         name|age|gender|  birthday|  zip|       city|\n",
            "+----------+-------------+---+------+----------+-----+-----------+\n",
            "|C007YEYTX9| Aaron Abbott| 34|Female| 7/13/1991|97823|     boston|\n",
            "|C00B971T1J| Aaron Austin| 37|Female|12/16/2004|30332|    chicago|\n",
            "|C00WRSJF1Q| Aaron Barnes| 29|Female| 3/11/1977|23451|     denver|\n",
            "|C01AZWQMF3|Aaron Barrett| 31|  Male|  7/9/1998|46613|los_angeles|\n",
            "|C01BKUFRHA| Aaron Becker| 54|  Male|11/24/1979|40284|  san_diego|\n",
            "+----------+-------------+---+------+----------+-----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Narrow** **Transoformation**"
      ],
      "metadata": {
        "id": "5AG4gTHZ-xjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_dataset = (customer_df\n",
        "                    .filter(F.col(\"city\") == \"boston\")\n",
        "                    .withColumn(\"first_name\", F.split(F.col(\"name\"), \" \")[0])\n",
        "                    .withColumn(\"last_name\", F.split(F.col(\"name\"), \" \")[1])\n",
        "                    .drop(\"name\")\n",
        "                    .withColumnRenamed(\"first_name\", \"name\")\n",
        "                    .withColumnRenamed(\"last_name\", \"surname\")\n",
        "                    .select(\"cust_id\", \"name\", \"surname\", \"city\")\n",
        "                    )\n",
        "\n",
        "customer_dataset.show(5)\n",
        "customer_dataset.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiBVb6-4--Ev",
        "outputId": "0a7d5916-2fb4-4988-ce77-af34dd6f2a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------+------+\n",
            "|   cust_id| name| surname|  city|\n",
            "+----------+-----+--------+------+\n",
            "|C007YEYTX9|Aaron|  Abbott|boston|\n",
            "|C08XAQUY73|Aaron| Lambert|boston|\n",
            "|C094P1VXF9|Aaron| Lindsey|boston|\n",
            "|C097SHE1EF|Aaron|   Lopez|boston|\n",
            "|C0DTC6436T|Aaron|Schwartz|boston|\n",
            "+----------+-----+--------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['cust_id, 'name, 'surname, 'city]\n",
            "+- Project [cust_id#82, age#84, gender#85, birthday#86, zip#87, city#88, name#270, last_name#252 AS surname#279]\n",
            "   +- Project [cust_id#82, age#84, gender#85, birthday#86, zip#87, city#88, first_name#243 AS name#270, last_name#252]\n",
            "      +- Project [cust_id#82, age#84, gender#85, birthday#86, zip#87, city#88, first_name#243, last_name#252]\n",
            "         +- Project [cust_id#82, name#83, age#84, gender#85, birthday#86, zip#87, city#88, first_name#243, split(name#83,  , -1)[1] AS last_name#252]\n",
            "            +- Project [cust_id#82, name#83, age#84, gender#85, birthday#86, zip#87, city#88, split(name#83,  , -1)[0] AS first_name#243]\n",
            "               +- Filter (city#88 = boston)\n",
            "                  +- Relation [cust_id#82,name#83,age#84,gender#85,birthday#86,zip#87,city#88] parquet\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "cust_id: string, name: string, surname: string, city: string\n",
            "Project [cust_id#82, name#270, surname#279, city#88]\n",
            "+- Project [cust_id#82, age#84, gender#85, birthday#86, zip#87, city#88, name#270, last_name#252 AS surname#279]\n",
            "   +- Project [cust_id#82, age#84, gender#85, birthday#86, zip#87, city#88, first_name#243 AS name#270, last_name#252]\n",
            "      +- Project [cust_id#82, age#84, gender#85, birthday#86, zip#87, city#88, first_name#243, last_name#252]\n",
            "         +- Project [cust_id#82, name#83, age#84, gender#85, birthday#86, zip#87, city#88, first_name#243, split(name#83,  , -1)[1] AS last_name#252]\n",
            "            +- Project [cust_id#82, name#83, age#84, gender#85, birthday#86, zip#87, city#88, split(name#83,  , -1)[0] AS first_name#243]\n",
            "               +- Filter (city#88 = boston)\n",
            "                  +- Relation [cust_id#82,name#83,age#84,gender#85,birthday#86,zip#87,city#88] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [cust_id#82, split(name#83,  , -1)[0] AS name#270, split(name#83,  , -1)[1] AS surname#279, city#88]\n",
            "+- Filter (isnotnull(city#88) AND (city#88 = boston))\n",
            "   +- Relation [cust_id#82,name#83,age#84,gender#85,birthday#86,zip#87,city#88] parquet\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [cust_id#82, split(name#83,  , -1)[0] AS name#270, split(name#83,  , -1)[1] AS surname#279, city#88]\n",
            "+- *(1) Filter (isnotnull(city#88) AND (city#88 = boston))\n",
            "   +- *(1) ColumnarToRow\n",
            "      +- FileScan parquet [cust_id#82,name#83,city#88] Batched: true, DataFilters: [isnotnull(city#88), (city#88 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/Spark/data/data_skew/customers.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,city:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZuI2lXU_qjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkIvASi-QNdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "orq-WPKCQN_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6be46f2b",
        "outputId": "601edcb2-3682-4bc4-b21e-631349f53eed"
      },
      "source": [
        "!git clone https://github.com/jvkvasanth/Spark.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spark'...\n",
            "remote: Enumerating objects: 362, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 362 (delta 2), reused 5 (delta 1), pack-reused 354 (from 2)\u001b[K\n",
            "Receiving objects: 100% (362/362), 702.53 MiB | 23.21 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Updating files: 100% (340/340), done.\n"
          ]
        }
      ]
    }
  ]
}